# 9장 웹 크롤러 설계

### 웹 크롤러
- 로봇 또는 스파이더 라고도 부름
- 검색 엔진에서 널리 쓰느 기술, 웹이 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적

### 웹 크롤러 과정

[사진 9-1]

### 웹 크롤러 이용

- 검색 엔진 인덱싱
  - 크롤러의 가장 보편적인 용례
  - Googlebot
- 웹 아카이빙
  - 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차
  - 미국 국회 도서관, EU 웹 아카이브
- 웹 마이닝
  - 웹의 폭발적 성장세로 인터넷에서 유용한 지식을 도출해 내는 것
  - 주주 총회 자료, 연차 보고서
- 웹 모니터링
  - 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링 하는 것
  - 디지마크

### 웹 크롤러의 복잡도
- 웹 크롤러가 처리해야 하는 데이터의 규모에 따라 달라짐

#

## 웹 크롤러 기본 알고리즘

### 1단계 - 문제 이해 및 설계 범위 확정

1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드
2. 다운받은 웹 페이지에서 URL들을 추출
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 반복

❓ 진짜로 이렇게 간단하게 동작할까?
- 엄청난 규모 확장성을 갖는 웹 크롤러를 설계한는 것은 엄청나게 어려운 작업

**[면접관의 크롤러 기능 요구사항]**
```bash
💡 이 크롤러의 주된 용도는 무엇인가요?
➡️ 검색 엔진 인덱싱에 쓰일 것입니다.

💡 매달 얼마나 많은 웹 페이지를 수집해야 하나요?
➡️ 10억개의 웹 페이지를 수집해야 합니다.

💡 수집한 웹 페이지는 저장해야 합니까?
➡️ 네, 5년간 저장해 두어야 합니다.

💡 중복된 콘텐츠는 어떻게 해야 하나요?
➡️ 중복된 콘텐츠를 갖는 페이지는 무시해도 됩니다.
```

➡️ 이러한 질문들을 통해 요구사항을 알아내고 모호한 부분을 제거해야 함

**[웹 크롤러가 만족시켜야 할 속성]**

```bash
⭐️ 규모 확장성
 ➡️ 병행성을 활용하면 보다 효과적으로 웹 크롤러 가능

⭐️ 안정성
 ➡️ 비정상적인 입력이나 환경에 잘 대응할 수 있어야 함

⭐️ 예절
 ➡️ 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안 됌

⭐️ 확장성
 ➡️ 새로운 형태의 콘텐츠를 지원하기가 쉬워야 함
```
### 계략적 규모 추정

아래의 추정치는 많은 가정으로 나온 것

- 매달 10억 개의 웹 페이지를 다운로드한다.
- QPS = 10억 (대략 400페이지/초)
- 최대(Peak) QPS = 2 X QPS = 800
- 웹 페이지의 크기 평균 500k라고 가정
- 10억 페이지 X 500k = 500TB/월
- 1개월치 데이터를 보내는데 500TB, 5년간 보관한다고 가정하면 결국 500TB X 12개월 X 5년 = 30PB의 저장용량 필요

#

### 2단계 - 계략적 설계안 제시 및 동의 구하기

[그림 9-4]

1. 시작 URL들을 미수집 URL 저장소에 저장한다.
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다.
3. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL의 IP 주소를 알아내고, 해당 IP 주소로 접속하여 웹 페이지를 다운받는다.
4. 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다.
5. 콘텐츠 파싱과 검증이 끝나면 중복 콘텐츠인지 확인하는 절차를 개시한다.
6. 중복 콘텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지 본다.
- 이미 저장소에 있는 콘첸츠인 경우에는 처리하지 않고 버린다.
- 저장소에 없는 콘텐츠인 경우에는 저장소에 저장한 뒤 URL 추출기로 전달한다.
7. URL 추출기는 해당 HTML 페이지에서 링크를 골라낸다.
8. 골라낸 링크를 URL 필터로 전달한다.
9. 필터링이 끝나고 남은 URL만 중복 URL 판별 단계로 전달한다.
10. 이미 처리한 URL인지 확인하기 위하여, URL 저장소에 보관된 URL인지 살핀다. 이미 저장소에 있는 URL은 버린다.
11. 저장소에 없는 URL은 URL 저장소에 저장할 뿐 아니라 미수집 URL 저장소에도 전달한다.

❓ 시작 URL
- 웹 크롤러가 크롤링을 시작하는 출발점
- 크롤러가 가능한 많은 링크를 탐색할 수 있도록 하는 URL을 고르는 것이 바람직

❓ 미수집 URL 저장소
- 다운로드할 URL을 저장/관리 하는 컴포넌트
- FIFO 큐

❓ 중복된 콘텐츠
- 두 HTML 문서를 비교하는 방법
  - 두 문서의 문자열을 비교 (비교 대상 문서의 수가 적을 경우)
  - 웹 페이지의 해시 값을 비교 (비교 대상 문서의 수가 많을 경우)

❓ URL 필터
- 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할
#

### 3단계 - 상세 설계

#### ❓ DFS를 쓸 것인가, BFS를 쓸 것인가

웹은 유향 그래프와 같다. 페이지는 노드이고, 하이퍼링크는 엣지라고 보면됨.

크롤링 프로세스는 이 유향 그래프를 엣지를 따라 탐색하는 과정.

DFS (깊이 우선 탐색법)
- 좋은 선택이 아닐 가능성 ⬆️
- 그래프 크기가 클 경우 어느 정도로 깊숙이 가게 될지 가늠하기 어렵기 때문

BFS (너비 우선 탐색법)
- 웹 크롤러는 보통 이 방법을 사용
- 그럼에도 두 가지 문제 존재
  - 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아감
    - [그림 9-5]
    - '예의 없는 크롤러'
  - 표준적 BFS 알고리즘은 URL 간에 우선순위를 두지 않음.

#### ❓ 미수집 URL 저장소
이 저장소를 잘 구현하면 '예의'를 갖춘 크롤러, URL 사이의 우선순위와 신선도를 구별하는 크롤러를 구현할 수 있음

#### 예의
- '무례한' 일
  - 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 보내는 것
  - 때로는 Dos 공격으로 간주되기도 함
- '예의있는' 일
  - 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청하는 것
  - 웹사이트의 호스트명과 다운로드를 수행하는 작접 스레드 사이의 관계를 유지하면 가능

#### 우선순위
크롤러 입장에서는 중요한 페이지를 먼저 수집하도록 하는 것이 바람직함

유용성에 따라 URL의 우선순위를 나눌 때
- 페이지랭크
- 트래픽 양
- 갱신 빈도
- etc..

#### 신선도
웹 페이지는 수시로 추가되고, 삭제되고 변경 됌

데이터의 신선함을 유지하기 위해서는 이미 다운로드한 페이지라고 해도 주기적으로 재수집 할 필요가 있음

- 웹 페이지의 변경 이력 활용
- 우선순위를 활용하여, 중요한 페이지는 좀 더 자주 재수집

#### 미수집 URL 저장소를 위한 지속성 저장장치
- 대부분의 URL은 디스크에 두지만 IO 비용을 줄이기 위해 메모리 버퍼에 큐를 두는 것

#### ❓ HTML 다운로더

#### Robots.txt
- 로봇 제외 프로토콜
- 웹사이트가 크롤러와 소통하는 표준적 방법

#### 성능 최적화
1. 분산 크롤링
- 성능을 높이기 위해 크롤링 작업을 여러 서버에 분산하는 방법
2. 도메인 이름 변환 결과 캐시
- DNS 요청이 처리되는 데는 보통 10ms ~ 200ms가 소요
- 크롤러 스레드 가운데 어느 하나라도 이 작업 중이면 다른 스레드의 DNS 요청은 모두 블록
3. 지역성
- 크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법
- 다운로드 시간을 줄일 수 있음
4. 짧은 타임아웃
- 어떤 웹 서버는 응답이 느리거나 아예 응답하지 않음
- 대기 시간이 길어지면 좋지 않으므로, 미리 정해두는 것이 좋음

#### 안정성
- 안정 해시
  - 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술
  - 다운로더 서버를 쉽게 추가하고 삭제 가능

- 크롤링 상태 및 수집 데이터 저장
  - 장애가 발생한 경우에도 쉽게 복구 가능

- 예외 처리
  - 에러는 불가피할 뿐만 아니라 흔하게 벌어지는 일
  - 예외가 발생해도 전체 시스템이 중단되는 일이 없어야 함

- 데이터 검증
  - 시스템 오류를 방지하기 위한 중요 수단 가운데 하나

#### 확장성
- 새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 신경 써야 함

[그림 9-10]

#### 문제 있는 콘텐츠 감지 및 회피

1. 중복 콘텐츠
2. 거미 덫
  - 크롤러는 무한 루프에 빠뜨리도록 설계한 웹 페이지
3. 데이터 노이즈

#

### 4단계 - 마무리

- 서버 측 랜더링
- 원치 않는 페이지 필터링
- 데이터베이스 다중화 및 샤딩
- 수평적 규모 확장성
- 가용성, 일관성, 안정성
- 데이터 분석 솔루션